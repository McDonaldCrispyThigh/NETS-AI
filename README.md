# NETS Business Data Enhancement System
## Employee Estimation and Survival Detection for Quick Service Restaurants and Pharmacies in Minneapolis

**Project Goal**: Develop an end-to-end pipeline to optimize the NETS business database by integrating multi-source data for employee count estimation and business survival probability prediction.

**Geographic Scope**: Minneapolis, Minnesota (Census Tract boundaries)  
**Industry Focus**: NAICS 722513 (Quick Service Restaurants) and NAICS 446110 (Pharmacies)  
**Target Sample Size**: 500-1000 establishments (MVP scale)  
**Timeline**: Phase 1 Complete - January 2026

---

## Quick Start

```powershell
# 1. Activate virtual environment
.\AIAGENTNETS\Scripts\Activate.ps1

# 2. Validate environment
python scripts/validate_environment.py

# 3. Test execution (2 businesses, skip GPT for speed)
python scripts/03_complete_pipeline.py --limit 2 --skip-gpt

# 4. Full collection with GPT analysis
python scripts/03_complete_pipeline.py --task coffee --limit 50

# 5. View results
# CSV: data/processed/ai_bdd_Minneapolis_coffee_*.csv
# Reviews: data/reviews/[place_id]_reviews.json
```

**Documentation**:
- [Quick Start Guide](docs/QUICKSTART.md)
- [API Cost Analysis](docs/api_costs_breakdown.md)
- [System Reference](docs/SYSTEM_REFERENCE.md)
- [Implementation Status](docs/IMPLEMENTATION_STATUS.md)

---

## Core Objectives

### 1. Employee Count Estimation with Confidence Intervals
Estimate employee counts using multi-signal integration with quantified uncertainty:
- **Data Sources**: LinkedIn company profiles + job posting databases + Google review velocity + street view business metrics
- **Methods**: XGBoost regression + Bayesian hierarchical modeling + bootstrap confidence intervals
- **Output**: Point estimates with 95% confidence intervals at monthly granularity
- **Validation**: Cross-check with LinkedIn public headcount data where available

### 2. Business Survival Detection
Identify operational status and detect closures earlier than NETS data updates (typical 24-month lag):
- **Data Sources**: Google review recency + review decay rate + job posting activity + street view imagery
- **Methods**: Random forest classification + decay curve analysis + computer vision door detection
- **Output**: Survival probability score (0-1) with confidence level (high/medium/low)
- **Validation**: Historical Wayback Machine snapshots + manual sampling of 100 businesses

### 3. Optimized Output Database
Generate cleaned, enriched Parquet database suitable for urban planning and economic analysis:
- **Format**: Apache Parquet (columnar, compressed, versioned)
- **Records**: Original NETS fields + optimized employee estimates + confidence intervals + survival probability
- **Geographic Accuracy**: Address parsing (usaddress library) + coordinate clustering + haversine distance validation
- **Quality Assurance**: All fields validated before export

---

## Pipeline Architecture

```
Data Ingestion Layer
â”œâ”€ NETS Database (establishments.csv)
â”œâ”€ Outscraper Google Reviews (1000 query/month limit)
â”œâ”€ LinkedIn Company Profiles (Selenium scraping + compliance)
â”œâ”€ Indeed Job Postings (historical trends)
â”œâ”€ OpenStreetMap Building Footprints (density analysis)
â””â”€ Google Street View (facade width measurement via OpenCV)

Data Cleaning and Standardization
â”œâ”€ Address parsing (usaddress library, handle variations)
â”œâ”€ Coordinate normalization (EPSG:4326 WGS84)
â”œâ”€ Name matching (fuzzy string matching, threshold < 50m haversine)
â”œâ”€ Deduplication (multikey: address + name + coordinates)
â””â”€ Temporal alignment (monthly period aggregation)

Feature Engineering
â”œâ”€ Review-based features:
â”‚  â”œâ”€ review_count_3m: review count in recent 3 months
â”‚  â”œâ”€ review_count_6_12m: review count in 6-12 months prior
â”‚  â”œâ”€ review_decay_rate: (count_3m / count_6_12m) - measures business decline
â”‚  â””â”€ days_since_last_review: recency indicator
â”œâ”€ Job posting features:
â”‚  â”œâ”€ posting_count_6m: recent 6-month job postings
â”‚  â”œâ”€ posting_peak_historical: maximum postings in any 6-month period
â”‚  â””â”€ hiring_activity_ratio: recent / historical
â”œâ”€ Street view features:
â”‚  â”œâ”€ facade_width_m: measured via edge detection (OpenCV)
â”‚  â”œâ”€ visible_signage: boolean presence
â”‚  â””â”€ window_lighting: activity proxy
â””â”€ OSM features:
   â”œâ”€ building_area_sqm: from OSM footprints
   â””â”€ district_density: nearby establishments per sq km

Model Development
â”œâ”€ Employee Count Regression:
â”‚  â”œâ”€ Features: review velocity + hiring activity + building area + OSM density
â”‚  â”œâ”€ Model: XGBoost (boosting with categorical features)
â”‚  â”œâ”€ Hierarchical Prior: PyMC Bayesian layers by NAICS code
â”‚  â””â”€ Uncertainty: bootstrap resampling for 95% confidence intervals
â”œâ”€ Survival Classification:
â”‚  â”œâ”€ Target labels: Active/Inactive (Wayback + manual validation)
â”‚  â”œâ”€ Features: review decay + posting activity + latest_review_age
â”‚  â”œâ”€ Model: Random Forest (interpretable split importance)
â”‚  â””â”€ Output: probability score (0-1) + confidence (high/medium/low)
â””â”€ Signal Fusion:
   â””â”€ Hard signals (LinkedIn): highest priority if available
   â””â”€ Soft signals: review data + CV metrics, weighted by recency

Output Generation
â”œâ”€ Parquet Database:
â”‚  â”œâ”€ Original NETS columns (preserved)
â”‚  â”œâ”€ employees_optimized: point estimate
â”‚  â”œâ”€ employees_ci_lower/upper: 95% confidence bounds
â”‚  â”œâ”€ is_active_prob: survival probability (0-1)
â”‚  â”œâ”€ confidence_level: high/medium/low categorical
â”‚  â”œâ”€ data_quality_score: 0-100 composite
â”‚  â””â”€ last_updated: timestamp
â””â”€ Streamlit Dashboard:
   â”œâ”€ Folium heat maps (by census tract)
   â”œâ”€ Temporal series (Altair): employee trends by NAICS
   â”œâ”€ Anomaly detection: outlier establishments
   â””â”€ Export tools: filtered CSV download
```

---

## Key Data Pipelines

### Data Priority Hierarchy
1. **NETS Database** (primary): Baseline establishment records
2. **Outscraper Google Reviews** (1000 queries/month limit): Review velocity + recency signals
3. **LinkedIn Company Profiles**: Employee counts (hard signal, highest credibility)
4. **Indeed Job Postings**: Hiring activity indicators
5. **OpenStreetMap**: Building footprint area + density metrics
6. **Google Street View**: Facade measurement + storefront visibility

### Data Cleaning Standards
- **Address Standardization**: usaddress parsing with fuzzy matching for coordinate alignment
- **Geographic Validation**: Haversine distance <50m for name+address matching
- **Temporal Alignment**: Monthly period aggregation (pd.to_period('M'))
- **Deduplication**: Multi-key matching (address + name + coordinates)
- **EPSG:4326**: All coordinates in WGS84 (required for geopandas)

### Uncertainty Quantification
- **Employee Count**: 95% confidence intervals via bootstrap resampling
- **Survival Probability**: Model prediction probability with categorical confidence
- **Data Quality Score**: Composite metric (0-100) based on signal completeness

---

## Repository Structure

```text
AI-BDD/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies (outscraper, playwright, googlemaps, etc.)
â”œâ”€â”€ .env                         # API keys (git-ignored, see .env.example)
â”œâ”€â”€ .gitignore                   # Git exclusion rules
â”œâ”€â”€ LICENSE                      # MIT License
â”œâ”€â”€ AIAGENTNETS/                 # Virtual environment (Python 3.14.2)
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_crane_decker_replication.ipynb
â”‚   â”œâ”€â”€ 02_minneapolis_pilot.ipynb
â”‚   â””â”€â”€ 03_statistical_validation.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py                # City configs + service category baselines
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ google_maps_agent.py         # Adaptive grid search (recursive subdivision)
â”‚   â”‚   â”œâ”€â”€ outscraper_agent.py          # Unlimited review collection + timeseries extraction
â”‚   â”‚   â”œâ”€â”€ linkedin_scraper_improved.py # 90-sec timeout LinkedIn scraper
â”‚   â”‚   â”œâ”€â”€ wayback_agent.py             # Internet Archive first/last snapshot
â”‚   â”‚   â””â”€â”€ gpt_analyzer.py              # GPT-4o-mini with full review context
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ sos_loader.py                # MN Secretary of State registry
â”‚   â”‚   â”œâ”€â”€ external_signals.py          # LinkedIn/Jobs/Popular Times (optional)
â”‚   â”‚   â””â”€â”€ validator.py                 # Output validation
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ employee_estimator.py        # Multi-signal + service-category logic
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ helpers.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                              # Input data (git-ignored)
â”‚   â”œâ”€â”€ processed/                        # CSV outputs (ai_bdd_*.csv)
â”‚   â”œâ”€â”€ reviews/                          # JSON review timeseries ([place_id]_reviews.json)
â”‚   â””â”€â”€ outputs/                          # Figures for paper
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_export_nets_snapshot.py
â”‚   â”œâ”€â”€ 02_run_minneapolis_pilot.py
â”‚   â”œâ”€â”€ 03_complete_pipeline.py          # Main data collection script
â”‚   â””â”€â”€ 03_generate_paper_figures.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_agents.py
â”‚   â””â”€â”€ test_validator.py
â””â”€â”€ docs/
    â”œâ”€â”€ QUICKSTART.md
    â”œâ”€â”€ IMPLEMENTATION_STATUS.md
    â”œâ”€â”€ api_costs_breakdown.md
    â””â”€â”€ SYSTEM_REFERENCE.md
```

---

## Quick Start Guide

### Prerequisites
- **Python 3.14.2** (current AIAGENTNETS venv version)
- **Git** for version control
- **Windows PowerShell 5.1+**
- **API Keys**:
  - OpenAI API (GPT-4o-mini for business analysis)
  - Google Maps API (Places + Geocoding)
  - Outscraper API (unlimited review collection, optional but recommended)
  - LinkedIn credentials (optional, for employee validation)

### Installation (3 minutes)

```powershell
# 1. Clone repository
git clone https://github.com/YourUsername/NETS-AI.git
cd NETS-AI

# 2. Activate existing virtual environment
.\AIAGENTNETS\Scripts\Activate.ps1

# 3. Install/update dependencies (if needed)
pip install -r requirements.txt

# 4. Set up environment configuration
# Create .env file with your API keys (see Configuration section below)
```

### Configuration

Create `.env` file in project root:

```env
# === REQUIRED API Keys ===
OPENAI_API_KEY=sk-proj-...              # GPT-4o-mini for business analysis
GOOGLE_MAPS_API_KEY=AIza...             # Google Maps Places API

# === RECOMMENDED (for unlimited reviews) ===
OUTSCRAPER_API_KEY=your_outscraper_key  # 97% cheaper than Google Maps API
                                        # Get free trial: https://outscraper.com/

# === OPTIONAL (for employee validation) ===
LINKEDIN_EMAIL=your@email.com           # LinkedIn scraper (90-sec timeout)
LINKEDIN_PASSWORD=your_password         # Requires saved session file

# === Project Settings ===
DATA_PATH=./data
LOG_LEVEL=INFO
```

### Run Minneapolis Coffee Shop Pilot

```powershell
# Test with 2 businesses (fast, skips GPT analysis)
python scripts/03_complete_pipeline.py --limit 2 --skip-wayback --skip-gpt

# Small batch with full analysis
python scripts/03_complete_pipeline.py --limit 10

# Full Minneapolis coffee shops (all ZIP codes)
python scripts/03_complete_pipeline.py --task coffee

# Results:
# - CSV: data/processed/ai_bdd_Minneapolis_coffee_YYYYMMDD_HHMMSS.csv
# - Reviews: data/reviews/[place_id]_reviews.json (one file per business)
# - Logs: logs/AI-BDD-Pipeline.log
```

### Output Structure

**CSV Columns** (43 fields):
- Basic: `name`, `address`, `phone`, `website`, `google_url`, `latitude`, `longitude`
- Reviews: `oldest_review_date`, `latest_review_date`, `total_reviews_collected`, `reviews_per_month`
- Wayback: `wayback_first_snapshot`, `wayback_last_snapshot`, `wayback_snapshot_count`
- Employees: `employee_estimate`, `employee_estimate_min`, `employee_estimate_max`, `employee_estimate_methods`
- AI Analysis: `ai_status`, `ai_status_confidence`, `ai_employees_estimate`

**Review JSON** (`data/reviews/ChIJxxx_reviews.json`):
```json
{
  "place_id": "ChIJxxx",
  "name": "Business Name",
  "collection_date": "2026-01-29T19:33:13",
  "reviews": [
    {
      "review_timestamp": 1528145483,
      "review_datetime_utc": "2018-06-04T20:51:23",
      "review_text": "Great service...",
      "review_rating": 5,
      "review_likes": 0
    }
  ],
  "statistics": {
    "oldest_review_date": "2018-06-04",
    "latest_review_date": "2025-12-24",
    "total_reviews": 400,
    "reviews_per_month": 5.2
  }
}
```

---

## Pipeline Architecture

### Stage 1: Adaptive Grid Search
```
ZIP Code â†’ Geocode Center â†’ 3Ã—3 Grid â†’ Search Each Cell
                                          â†“
                                  â‰¥55 results? â†’ Subdivide into 4 quadrants (recursive)
                                          â†“
                                  <55 results â†’ Deduplicate by place_id â†’ Next cell
```

### Stage 2: Full Data Collection (per business)
```
Place ID â†’ Google Maps Details â†’ Outscraper Reviews (unlimited)
                                          â†“
                                  Save to data/reviews/[place_id]_reviews.json
                                          â†“
                                  Extract statistics â†’ CSV
```

### Stage 3: AI Analysis (optional, --skip-gpt to disable)
```
Load full reviews â†’ GPT-4o-mini analyzes:
  - Business status (Active/Inactive/Uncertain)
  - Employee estimate (review density + staff mentions)
  - NAICS verification (menu/service evolution)
```

### Stage 4: Employee Estimation (batch processing)
```
Calculate industry baseline (avg reviews/month)
For each business:
  - Service category? â†’ Review density + Popular Times only
  - Other category? â†’ LinkedIn + Job postings + Building area + Review density + Popular Times + SOS partners
  â†’ Average valid signals â†’ employee_estimate
```

---

## Data Sources & API Costs

| Data Source | Purpose | Cost | Coverage |
|------------|---------|------|----------|
| **Google Maps Places API** | Initial search + place details | $0.032/place | All categories |
| **Outscraper** | Unlimited reviews (0=all) | $0.001/place | 97% cheaper than Google |
| **Wayback Machine CDX API** | Historical validation (free) | $0 | 800B+ snapshots |
| **OpenAI GPT-4o-mini** | Business status + employee AI analysis | $0.150/1M input tokens | All text |
| **LinkedIn (optional)** | Employee count validation | $0 (scraping) | Limited coverage |

**Minneapolis Coffee Shop Pilot Cost** (250 businesses):
- Google Maps: $8.00
- Outscraper reviews: $0.25
- GPT-4o-mini: $2.50
- **Total: ~$11 per 250 businesses**

Compare to NETS: $50,000+/year for national coverage

---

## Validation Strategy

### 1. Consistency Test (Reproducibility)
- Run pipeline 3Ã— on same ZIP code
- Calculate Jaccard similarity: `|Aâˆ©B|/|AâˆªB|` for place_id sets
- **Target**: â‰¥0.95 similarity (current: 0.96-0.98 depending on timing)

### 2. External Validation
- **MN SOS Registry**: Cross-check active businesses (incorporation date)
- **OpenStreetMap**: Compare POI coverage (completeness metric)
- **Manual Ground Truth**: Field visit 50 random locations (precision/recall)

### 3. NETS Comparison
- **Interpolation**: Compare AI-BDD employee volatility (Gini) vs. NETS smoothness
- **Zombie Lag**: Closure detection time (AI-BDD: 3-6mo vs. NETS: 24+mo)
- **2011 Spikes**: Wayback validation of "opened 2011" â†’ flag artifacts
- **Implicit Rounding**: Review density confidence intervals vs. NETS point estimates

---

## Key Implementation Details

### Adaptive Grid Search Logic
```python
def search_cell(lat, lng, radius_m, depth=0):
    results = places_nearby(lat, lng, radius_m)
    
    if len(results) >= 55 and depth < 3:
        # Subdivide into 4 quadrants (NE, NW, SE, SW)
        for quadrant in [(+offset, +offset), (+offset, -offset), 
                         (-offset, +offset), (-offset, -offset)]:
            search_cell(lat+quadrant[0], lng+quadrant[1], radius_m//2, depth+1)
    else:
        # Deduplicate and add to results
        for place in results:
            all_places[place['place_id']] = place
```

### Service Category Employee Estimation
```python
if category in SERVICE_CATEGORIES:
    # Use only review density + popular times
    review_intensity = reviews_per_month / industry_baseline
    employees_from_reviews = baseline_staff * review_intensity
    
    peak_customers = popular_times_peak * max_customers_per_hour
    employees_from_flow = peak_customers / 12.5  # 12.5 customers/staff
    
    estimate = avg(employees_from_reviews, employees_from_flow)
else:
    # Use full multi-signal model
    estimate = avg(linkedin, job_postings, building_area, 
                   review_density, popular_times, sos_partners)
```

---

## Current Implementation Status

âœ… **Completed**:
- Adaptive grid search with recursive subdivision (100% coverage)
- Outscraper unlimited review collection (`reviews_limit=0`)
- Review timeseries storage (separate JSON files)
- GPT-4o-mini full review analysis (all reviews, not just 5)
- Service-category employee estimation (review density + Popular Times)
- Wayback Machine historical validation
- Multi-signal employee estimator with confidence intervals
- Pipeline CSV output with 43 fields

ðŸš§ **In Progress**:
- Minneapolis full pilot (coffee shops + gyms)
- Consistency validation (3Ã— run comparison)
- NETS snapshot export for direct comparison

ðŸ“‹ **Planned**:
- Computer Vision: Street View storefront size estimation
- OSM POI cross-validation
- Statistical validation notebooks (Gini, ROC curves)
- Paper figures generation script

---

## Troubleshooting

### Common Issues

**"Outscraper reviews error"**
- Ensure `OUTSCRAPER_API_KEY` is set in `.env`
- Falls back to Google Maps API (only 5 reviews) if Outscraper unavailable

**"LinkedIn scraper timeout"**
- Increase timeout in `linkedin_scraper_improved.py` (currently 90 seconds)
- Requires valid session file or will skip LinkedIn data

**"Grid search returns <60 results but incomplete"**
- Google Maps API may have regional coverage gaps
- Cross-validate with OpenStreetMap for completeness

**"CSV formatting issues"**
- Review timeseries now stored separately (not in CSV)
- Check `data/reviews/` for individual JSON files

---

## Key References

1. **Crane, L. D., & Decker, R. A. (2019).** *Business Dynamics in the National Establishment Time Series (NETS)*. Federal Reserve Working Paper. [Link](https://www.federalreserve.gov/econres/feds/files/2019034pap.pdf)

---

## Contributing

We welcome contributions! Please:
1. Fork this repository
2. Create a feature branch (`git checkout -b feature/your-feature`)
3. Submit a pull request with clear description

## License

MIT License - see LICENSE file for details

---

**Documentation Version**: Jan 29, 2026  
**Maintainer**: Congyuan (CU Boulder)  
**Contact**: [Your Email/GitHub Issues]

