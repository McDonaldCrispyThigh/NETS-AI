#!/usr/bin/env python3
"""
NETS Data Path Reference and Schema Documentation
==================================================
This file documents the required data paths and CSV schema for the NETS
Enhancement pipeline. NO SAMPLE DATA GENERATION - paths only.

Usage:
    python scripts/generate_sample_data.py

This will print:
    - Required input file paths
    - Required CSV column schema
    - Optional enrichment columns
    - Data file status check
"""

from pathlib import Path
import sys


# =============================================================================
# DATA PATH CONFIGURATION
# =============================================================================

# Project root (relative to this script)
PROJECT_ROOT = Path(__file__).parent.parent

# Input data paths
DATA_RAW_DIR = PROJECT_ROOT / "data" / "raw"
DATA_PROCESSED_DIR = PROJECT_ROOT / "data" / "processed"
DATA_OUTPUTS_DIR = PROJECT_ROOT / "data" / "outputs"
TEST_FIXTURES_DIR = PROJECT_ROOT / "tests" / "fixtures"

# Expected input files (user must provide)
NETS_INPUT_PATH = DATA_RAW_DIR / "nets_minneapolis.csv"
NETS_FULL_PATH = DATA_RAW_DIR / "nets_minneapolis_full.csv"
TEST_FIXTURE_PATH = TEST_FIXTURES_DIR / "nets_test_data.csv"

# Census tract boundaries (optional, for spatial join)
CENSUS_TRACTS_PATH = DATA_RAW_DIR / "census_tracts_2020.shp"


# =============================================================================
# REQUIRED CSV SCHEMA
# =============================================================================

REQUIRED_COLUMNS = {
    # Core Identifiers (REQUIRED)
    "duns_id": "str - Unique DUNS business identifier (primary key)",
    "company_name": "str - Business/establishment name",
    
    # Industry Classification (REQUIRED)
    "naics_code": "str - 6-digit NAICS code. Must be '722513' or '446110'",
    
    # Location - Coordinates (REQUIRED)
    "latitude": "float - WGS84 latitude (EPSG:4326). Example: 44.9778",
    "longitude": "float - WGS84 longitude (EPSG:4326). Example: -93.2650",
    
    # Location - Address (REQUIRED)
    "street_address": "str - Street address",
    "city": "str - City name (e.g., 'Minneapolis')",
    "state": "str - 2-letter state code (e.g., 'MN')",
    "zip_code": "str - 5-digit ZIP code",
}

OPTIONAL_NETS_COLUMNS = {
    # Additional NETS fields (recommended but not required)
    "naics_title": "str - NAICS description text",
    "sic_code": "str - Legacy SIC classification code",
    "phone": "str - Business phone number",
    "website": "str - Business website URL",
    "year_established": "int - Year business was established",
    "year_closed": "int - Year business closed (null if still active)",
    "employee_count_raw": "int - Raw employee count from NETS",
    "emp_here": "int - NETS employment at this location",
    "emp_total": "int - NETS total employment (HQ + branches)",
    "sales": "float - Annual sales figure",
    "company_id": "str - Alternative unique identifier",
    "year": "int - NETS snapshot year",
}

OPTIONAL_ENRICHMENT_COLUMNS = {
    # External signal columns (added by pipeline if APIs enabled)
    "linkedin_employee_count": "int - Employee count from LinkedIn profile",
    "review_count_3m": "int - Review count in last 3 months",
    "review_count_6_12m": "int - Review count 6-12 months ago",
    "last_review_date": "str - Date of most recent review (YYYY-MM-DD)",
    "job_postings_6m": "int - Job postings in last 6 months",
    "google_place_id": "str - Google Maps Place ID",
    "wayback_latest_snapshot": "str - Latest Wayback Machine snapshot date",
    "estimated_area_sqm": "float - Building footprint area in square meters",
}


# =============================================================================
# OUTPUT SCHEMA (Generated by Pipeline)
# =============================================================================

OUTPUT_COLUMNS = {
    # All input columns preserved, plus:
    "census_tract_id": "str - 11-digit FIPS tract code (computed via spatial join)",
    "employees_optimized": "float - Bayesian posterior mean estimate",
    "employees_lower_ci": "float - 95% CI lower bound (2.5th percentile)",
    "employees_upper_ci": "float - 95% CI upper bound (97.5th percentile)",
    "employees_confidence": "str - Categorical: 'high', 'medium', or 'low'",
    "is_active_prob": "float - Operational probability (0.0 to 1.0)",
    "is_active_confidence": "float - Prediction confidence score",
    "data_quality_score": "int - 0-100 quality score based on completeness",
    "last_updated": "datetime - Pipeline execution timestamp",
    "pipeline_version": "str - Semantic version (e.g., '0.4.2')",
}


# =============================================================================
# GEOGRAPHIC REFERENCE (Minneapolis)
# =============================================================================

MINNEAPOLIS_BOUNDS = {
    "lat_min": 44.8899,
    "lat_max": 45.0428,
    "lon_min": -93.3223,
    "lon_max": -93.1833,
}

MINNEAPOLIS_ZIP_CODES = [
    "55401", "55402", "55403", "55404", "55405",
    "55406", "55407", "55408", "55409", "55410",
    "55411", "55412", "55413", "55414", "55415",
]

TARGET_NAICS_CODES = {
    "722513": "Limited-Service Restaurants (Quick Service Restaurants)",
    "446110": "Pharmacies and Drug Stores",
}


# =============================================================================
# MAIN: Print Documentation and Check Status
# =============================================================================

def print_separator(title: str = "") -> None:
    """Print formatted section separator"""
    print("\n" + "=" * 80)
    if title:
        print(f"  {title}")
        print("=" * 80)


def print_path_status(path: Path, description: str) -> bool:
    """Check and print file/directory status"""
    exists = path.exists()
    status = "[OK]" if exists else "[--]"
    size_info = ""
    if exists and path.is_file():
        size_kb = path.stat().st_size / 1024
        if size_kb > 1024:
            size_info = f" ({size_kb/1024:.1f} MB)"
        else:
            size_info = f" ({size_kb:.1f} KB)"
    print(f"  {status} {path.relative_to(PROJECT_ROOT)}{size_info}")
    print(f"       {description}")
    return exists


def print_columns(columns: dict, indent: int = 2) -> None:
    """Print column schema with descriptions"""
    prefix = " " * indent
    for col, desc in columns.items():
        print(f"{prefix}- {col:30s}: {desc}")


def main() -> int:
    """Main entry point - print documentation and check data status"""
    
    print_separator("NETS ENHANCEMENT PIPELINE - DATA PATH REFERENCE")
    print("\n  This script documents required data paths and CSV schema.")
    print("  NO DATA GENERATION - you must provide your own NETS CSV files.")
    
    # ==========================================================================
    # Section 1: Directory Structure
    # ==========================================================================
    print_separator("DIRECTORY STRUCTURE")
    
    print("\n  Required directory layout:")
    print("""
    NETS-AI/
    +-- data/
    |   +-- raw/                    <- Place your NETS CSV files here
    |   |   +-- nets_minneapolis.csv
    |   |   +-- census_tracts_2020.shp (optional)
    |   +-- processed/              <- Pipeline outputs (Parquet)
    |   +-- outputs/                <- Validation reports, figures
    +-- tests/
        +-- fixtures/               <- Small test CSV (5-20 records)
            +-- nets_test_data.csv
    """)
    
    # ==========================================================================
    # Section 2: Required Input Files
    # ==========================================================================
    print_separator("REQUIRED INPUT FILES")
    
    print("\n  Production data (full NETS snapshot):")
    print_path_status(NETS_INPUT_PATH, "Primary input file for Minneapolis pilot")
    print_path_status(NETS_FULL_PATH, "Alternative: full dataset path")
    
    print("\n  Test fixtures (small subset for development):")
    print_path_status(TEST_FIXTURE_PATH, "5-20 records for --test mode")
    
    print("\n  Optional geographic data:")
    print_path_status(CENSUS_TRACTS_PATH, "Census tract boundaries for spatial join")
    
    # ==========================================================================
    # Section 3: Required CSV Schema
    # ==========================================================================
    print_separator("REQUIRED CSV COLUMNS (8 mandatory)")
    print_columns(REQUIRED_COLUMNS)
    
    print_separator("OPTIONAL NETS COLUMNS (recommended)")
    print_columns(OPTIONAL_NETS_COLUMNS)
    
    print_separator("ENRICHMENT COLUMNS (added by pipeline)")
    print_columns(OPTIONAL_ENRICHMENT_COLUMNS)
    
    # ==========================================================================
    # Section 4: Output Schema
    # ==========================================================================
    print_separator("OUTPUT COLUMNS (generated by pipeline)")
    print_columns(OUTPUT_COLUMNS)
    
    print("\n  Output format: Apache Parquet")
    print(f"  Output path:   {DATA_PROCESSED_DIR.relative_to(PROJECT_ROOT)}/nets_enhanced_<city>_<timestamp>.parquet")
    
    # ==========================================================================
    # Section 5: NAICS Codes
    # ==========================================================================
    print_separator("TARGET NAICS CODES")
    for code, desc in TARGET_NAICS_CODES.items():
        print(f"  - {code}: {desc}")
    
    # ==========================================================================
    # Section 6: Geographic Reference
    # ==========================================================================
    print_separator("MINNEAPOLIS GEOGRAPHIC REFERENCE")
    print(f"\n  Bounding Box (EPSG:4326):")
    print(f"    Latitude:  {MINNEAPOLIS_BOUNDS['lat_min']:.4f} to {MINNEAPOLIS_BOUNDS['lat_max']:.4f}")
    print(f"    Longitude: {MINNEAPOLIS_BOUNDS['lon_min']:.4f} to {MINNEAPOLIS_BOUNDS['lon_max']:.4f}")
    print(f"\n  Target ZIP Codes:")
    print(f"    {', '.join(MINNEAPOLIS_ZIP_CODES)}")
    
    # ==========================================================================
    # Section 7: Usage Instructions
    # ==========================================================================
    print_separator("USAGE INSTRUCTIONS")
    
    print("""
    1. PREPARE YOUR DATA:
       - Export NETS snapshot to CSV with required columns
       - Filter to NAICS codes 722513 and 446110
       - Ensure coordinates are in WGS84 (EPSG:4326)
    
    2. PLACE FILES:
       - Production: data/raw/nets_minneapolis.csv
       - Testing:    tests/fixtures/nets_test_data.csv (5-20 records)
    
    3. RUN PIPELINE:
       # Test mode (uses fixtures, no API calls)
       python scripts/run_pipeline.py --test
       
       # Production mode
       python scripts/run_pipeline.py --input data/raw/nets_minneapolis.csv
       
       # Skip optional components
       python scripts/run_pipeline.py --input data/raw/nets_minneapolis.csv --skip linkedin wayback
    
    4. CHECK OUTPUT:
       - Parquet: data/processed/nets_enhanced_Minneapolis_<timestamp>.parquet
       - Dashboard: streamlit run dashboard/app.py
    """)
    
    print_separator()
    print("  Data path documentation complete.")
    print("  Provide your NETS CSV files and run the pipeline.\n")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
